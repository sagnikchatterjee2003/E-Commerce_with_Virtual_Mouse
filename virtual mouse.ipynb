{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Virtual Mouse using Finger Tracking",
   "id": "c9557f241d1d9c62"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-21T16:25:19.528544Z",
     "start_time": "2025-07-21T16:25:16.596328Z"
    }
   },
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Configuration Constants",
   "id": "7973c7d44c5075c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T16:25:19.559739Z",
     "start_time": "2025-07-21T16:25:19.543918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "WEBCAM_ID = 0\n",
    "CAM_WIDTH = 640\n",
    "CAM_HEIGHT = 480\n",
    "SMOOTHING = 7\n",
    "FRAME_REDUCTION = 100"
   ],
   "id": "c6c78924c5284e4f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Main Function",
   "id": "a0405039e2ff94e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T16:28:55.588474Z",
     "start_time": "2025-07-21T16:28:55.572360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # Initialization\n",
    "    p_time = 0\n",
    "\n",
    "    # Previous and current locations for smoothing\n",
    "    p_loc_x, p_loc_y = 0, 0\n",
    "    c_loc_x, c_loc_y = 0, 0\n",
    "\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(WEBCAM_ID)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open webcam with ID {WEBCAM_ID}.\")\n",
    "        return\n",
    "    cap.set(3, CAM_WIDTH)\n",
    "    cap.set(4, CAM_HEIGHT)\n",
    "\n",
    "    # Get screen dimensions\n",
    "    screen_width, screen_height = pyautogui.size()\n",
    "    pyautogui.FAILSAFE = False # Disable the failsafe to prevent accidental program termination\n",
    "\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "    print(\"AI Virtual Mouse started. Show your hand to the camera.\")\n",
    "    print(\"Move your index finger to move the cursor.\")\n",
    "    print(\"Bring your middle finger close to your index finger to click.\")\n",
    "    print(\"Press 'q' to quit.\")\n",
    "\n",
    "    # Main Loop\n",
    "    try:\n",
    "        while True:\n",
    "            # Read a frame from the webcam\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                continue\n",
    "\n",
    "            # Flip the frame horizontally for a more intuitive, mirror-like experience\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Process the frame to find hand landmarks\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "\n",
    "            # If a hand is detected, process the landmarks\n",
    "            if results.multi_hand_landmarks:\n",
    "                hand_landmarks = results.multi_hand_landmarks[0] # Get landmarks for the first hand\n",
    "\n",
    "                # Get coordinates for index and middle finger tips\n",
    "                index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "\n",
    "                # Convert normalized landmark coordinates to frame coordinates\n",
    "                h, w, c = frame.shape\n",
    "                ix, iy = int(index_finger_tip.x * w), int(index_finger_tip.y * h)\n",
    "                mx, my = int(middle_finger_tip.x * w), int(middle_finger_tip.y * h)\n",
    "\n",
    "                # Map Hand Coordinates to Screen Coordinates\n",
    "                screen_x = np.interp(ix, (FRAME_REDUCTION, w - FRAME_REDUCTION), (0, screen_width))\n",
    "                screen_y = np.interp(iy, (FRAME_REDUCTION, h - FRAME_REDUCTION), (0, screen_height))\n",
    "\n",
    "                # Smooth the coordinates to reduce jitter\n",
    "                c_loc_x = p_loc_x + (screen_x - p_loc_x) / SMOOTHING\n",
    "                c_loc_y = p_loc_y + (screen_y - p_loc_y) / SMOOTHING\n",
    "\n",
    "                # Move the mouse\n",
    "                pyautogui.moveTo(c_loc_x, c_loc_y)\n",
    "                p_loc_x, p_loc_y = c_loc_x, c_loc_y # Update previous location\n",
    "\n",
    "                # Gesture Recognition: Check for click\n",
    "                # Calculate the distance between the index and middle finger tips\n",
    "                click_distance = math.hypot(mx - ix, my - iy)\n",
    "\n",
    "                # Draw visuals on the frame for feedback\n",
    "                cv2.circle(frame, (ix, iy), 15, (255, 0, 255), cv2.FILLED) # Circle on index finger\n",
    "\n",
    "                # If the distance is small, it's a click\n",
    "                if click_distance < 30: # Threshold distance for a click\n",
    "                    cv2.circle(frame, (ix, iy), 15, (0, 255, 0), cv2.FILLED) # Green circle indicates click\n",
    "                    pyautogui.click()\n",
    "                    time.sleep(0.2) # Small delay to prevent multiple clicks\n",
    "\n",
    "                # Draw all hand landmarks for visualization\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Display FPS\n",
    "            c_time = time.time()\n",
    "            fps = 1 / (c_time - p_time)\n",
    "            p_time = c_time\n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "\n",
    "            # Display the frame in a window\n",
    "            cv2.imshow(\"AI Virtual Mouse\", frame)\n",
    "\n",
    "            # Check for quit command\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        print(\"\\nShutting down...\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        hands.close()"
   ],
   "id": "ef654100bc6a0f65",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T16:39:11.043557Z",
     "start_time": "2025-07-21T16:29:06.414858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "db4ffdee4da88aca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Virtual Mouse started. Show your hand to the camera.\n",
      "Move your index finger to move the cursor.\n",
      "Bring your middle finger close to your index finger to click.\n",
      "Press 'q' to quit.\n",
      "\n",
      "Shutting down...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 49\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# Process the frame to find hand landmarks\u001B[39;00m\n\u001B[0;32m     48\u001B[0m frame_rgb \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2RGB)\n\u001B[1;32m---> 49\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mhands\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_rgb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# If a hand is detected, process the landmarks\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results\u001B[38;5;241m.\u001B[39mmulti_hand_landmarks:\n",
      "File \u001B[1;32mC:\\Projects\\Virtual_Mouse\\.venv\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001B[0m, in \u001B[0;36mHands.process\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[0;32m    133\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;124;03m         right hand) of the detected hand.\u001B[39;00m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Projects\\Virtual_Mouse\\.venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[1;34m(self, input_data)\u001B[0m\n\u001B[0;32m    366\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[0;32m    368\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[0;32m    369\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[0;32m    370\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[1;32m--> 372\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[0;32m    375\u001B[0m solution_outputs \u001B[38;5;241m=\u001B[39m collections\u001B[38;5;241m.\u001B[39mnamedtuple(\n\u001B[0;32m    376\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSolutionOutputs\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info\u001B[38;5;241m.\u001B[39mkeys())\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e83a14ea6297721"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
